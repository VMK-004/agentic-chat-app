# ğŸ§  Agentic Chat App

A real-time chat application built using **Flutter** and **Django Channels**, enhanced with an **Agentic AI Assistant** powered by **Ollama + RAG (Retrieval-Augmented Generation)**. This app supports **file sharing**, **AI-based summaries**, and **reply suggestions**, with a minimum UI.

---

## ğŸš€ Features

- ğŸ’¬ Real-time messaging via WebSocket
- ğŸ¤– Local AI Assistant (Ollama) integration
- ğŸ“ File upload and preview in chat
- ğŸ“„ AI-powered file summarization (PDFs)
- ğŸ’¡ AI reply suggestions and chat summarization
- ğŸ“‚ RAG system with FAISS & sentence-transformers
- ğŸ¯ Floating AI assistant UI in Flutter
- ğŸ¨ Clean UI with avatars, timestamps, and suggestions

---

## ğŸ§© Tech Stack

| Layer      | Technologies Used                                           |
| ---------- | ----------------------------------------------------------- |
| Frontend   | Flutter, Dart, Dio, FilePicker, WebSocket                   |
| Backend    | Django, Django Channels, FastAPI                            |
| AI Layer   | Ollama (local LLM), Langchain, FAISS, sentence-transformers |
| Realtime   | WebSocket, Redis (via Docker)                               |
| Storage    | Uploaded file caching & static file server                  |
| Deployment | Locally tested (to be deployed)                             |

---

## ğŸ“¸ Demo Preview

### ğŸ”¹ 1. Real-Time Chat + AI Suggestions (Part 1)

![Real-Time Chat + AI](Chat_Application.gif)

### ğŸ”¹ 2. Real-Time Chat + AI Suggestions (Part 2)

![File Sharing](</Chat_Application%20(1).gif>)

### ğŸ”¹ 3. Real-Time Chat + AI Suggestions (Part 3)

![File Summarization](</Chat_Application%20(2).gif>)

---

## ğŸš€ Getting Started

### ğŸ“¦ Prerequisites

Before running the project, make sure you have the following installed:

- **Flutter SDK** (for mobile app) â†’ [Install Flutter](https://docs.flutter.dev/get-started/install)
- **Python 3.8+** with `pip`
- **Ollama** with Mistral model installed â†’ [Install Ollama](https://ollama.com/)
- **Docker** (to run Redis server) â†’ [Install Docker](https://docs.docker.com/get-docker/)
- **Redis** (via Docker container)

---

### ğŸ”§ Backend Setup (Django + FastAPI)

````bash
# Clone the repo
git clone https://github.com/VMK-004/agentic-chat-app.git
cd agentic-chat-app

# Create a virtual environment
python -m venv venv
source venv/bin/activate        # macOS/Linux
venv\Scripts\activate           # Windows

# Install Python dependencies
pip install -r requirements.txt

# Run Redis using Docker
docker run -p 6379:6379 redis

# Start the Django backend server
cd chatproject
python manage.py runserver

# In a separate terminal, start the FastAPI AI server
cd ../agent_backend
uvicorn main:app --reload --port 8001


---

## ğŸ§  How the AI Works

### ğŸ“ Local Ollama LLM

Runs a lightweight qwen3:0.6b model locally for chat, summaries, and suggestions.

### ğŸ§  RAG Pipeline

- PDFs are embedded using sentence-transformers
- FAISS vector search retrieves relevant chunks
- Langchain orchestrates responses using LLM + context

---

## ğŸ“ Project Structure

```plaintext
Chat_Application/
â”œâ”€â”€ chat_flutter/         # Flutter frontend
â”‚   â””â”€â”€ lib/main.dart     # Main UI logic
â”œâ”€â”€ chatproject/          # Django backend for chat
â”‚   â””â”€â”€ consumers.py      # WebSocket consumer
â”œâ”€â”€ agent_backend/        # FastAPI backend for AI + RAG
â”‚   â””â”€â”€ main.py           # Handles summarization, AI chat
â”œâ”€â”€ uploaded_files/       # Cached user uploads
â””â”€â”€ README.md
````

## ğŸ‘¨â€ğŸ’» Author

- **Mohan Krishna** â€“ [LinkedIn](https://www.linkedin.com/in/vmkrishna2004/)<!-- | [Portfolio](https://yourportfolio.com) -->
